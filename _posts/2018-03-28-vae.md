# VAE

### The Variational Inference Problem 
Since I have read VAE for many many many times, I think it's better to just note down what I understand. 

In a fully bayesian setting, suppose there is a distribution, $p(\mathbf{z},\mathbf{x})$, where the $\mathbf{x}\sim p(\mathbf{x})$ is the data distribution, and the $\mathbf{z} \sim p(\mathbf{z})$ can represent any latent variables, for example, the components in a multinomial Gaussian, or, the parameters (this is usually the case) of the likelihood function, we want to know $p(\mathbf{x})$.

Let's take an example. Suppose we have in hand the data points $x_1, x_2,\ldots,x_n$, then we want to use a parametric function to approximate $f_\theta = p(\mathbf{x})$, instead of just counting points. In normal case, we simply use the maximum likelihood to compute $f_\theta$, i.e., $\hat{\theta} = \arg~\max _\theta p(\theta|x)$, but it's usually hard to find a single function $f$ which is suitable for high dimensional data (I guess), i.e., images. Another option is to employ a fully bayesian model. To avoid awkwardness, let $z$ be $\theta$, which means the parameters are the hidden variables that we want to include in the model.

Our model includes four elements: the prior $p(\mathbf{z})$, the likelihood $p(\mathbf{z}|\mathbf{x})$, the posterior $p(\mathbf{x}|z)$, and the generally interesting data distribution $p(\mathbf{x})$. Then by bayesian rule, 

$$
p(\mathbf{x}) = \frac{p(\mathbf{z})p(\mathbf{x}|\mathbf{z})}{p(\mathbf{z}|\mathbf{x})}
$$

Usually we assume the prior, the likelihood, then the item left is the posterior $p(\mathbf{z}|\mathbf{x})$. How to compute the posterior without knowing $p(\mathbf{x})$. This is the problem called *Variational Inference*.

### The Variational Lowerbound
We want to use a parametric function $q(\mathbf{z})$ to approximate the posterior, where we name it the variational distribution. We know the log marginal probability can be expressed by 
$$
\ln p(\mathbf{x}) = \mathcal{L}(q) + KL(q||p)
$$
where we have defined
$$
\mathcal{L}(q,\theta) = \sum_z q(\mathbf{z}) \ln \frac{p(\mathbf{x,z})}{q(\mathbf{z})};  
KL(q||p) = \sum_z q(\mathbf{z}) \ln \frac{q(\mathbf{z})}{p(\mathbf{z|x})}
$$
>PRML, p450, p465

Fixing the **LHS**, then as we maximize the variational lower bound $\mathcal{L}$, we are in face simultaneously minimizing the KL divergence. Also the lower bound can be written as:
$$
\mathcal{L} = KL(q(\mathbf{z|x})||p(\mathbf{z})) + \mathbb{E}_{q(\mathbf{z|x})}[\ln p(\mathbf{x|z})]
$$
> VAE, formular (3)

We call the first item the regularizor and the second item the point-wise distance. 

Then as long as we assume the parametric form of the prior, the likelihood, and the variational encoder, we are ready to maximize the **RHS** of the lower bound. Once we got the optimal $q_\sigma(z|x)$, (actually this is also the $q(\mathbf{z})$), we believe we've find the function of the true posterior.

In PRML, the assumption of the variational lower bound can be factorized (10.1.1), or be a univariate Gaussian (10.1.3). In the VAE paper, it can be a multivariate Gaussian. Then the assumption of the prior is also Guassian, and the likelihood can be also gaussian or just a Bernoulli. With these assumptions, we can analytically write out the KL divergence. 

One concern here can be that, why can we assume such a simple functional for the hidden variables and the data? This is more on tractability. Most of the time, we need to assume something that we can compute. The introduction of the hidden variable has enable extra flexibility for our model, which however too complicated hidden probability we cannot compute. On the other hand, with Neural Network, the universal approximator, we actually in theory can approximate any function already. Then, a.k.a to the kernel tricks in SVM, we are likewise to map from the data space to the assumed feature space by the powerful encoder, and the decoder. So whatever simply assumption on the likelihood and the prior, plus the powerful network, it is already a very complex, nonconvex function.

Now, how do we compute the point-wise distance?
